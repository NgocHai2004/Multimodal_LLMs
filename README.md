# Multimodal_LLMs
L√†m tr√™n kaggle 
D·ª± √°n 1: Image Captioning c∆° b·∫£n (Sinh m√¥ t·∫£ t·ª´ ·∫£nh)

S·ª≠ d·ª•ng m√¥ h√¨nh c√≥ s·∫µn nh∆∞ BLIP ho·∫∑c LLaVA

M·ª•c ti√™u:
Nh·∫≠p v√†o m·ªôt ·∫£nh ‚Üí m√¥ h√¨nh sinh ra m√¥ t·∫£ b·∫±ng ti·∫øng Vi·ªát ho·∫∑c ti·∫øng Anh.

C√¥ng c·ª•:
Python + transformers + torch

M√¥ h√¨nh: BLIP (Salesforce) ho·∫∑c blip-image-captioning-base t·ª´ Hugging Face

Dataset: MS-COCO (ho·∫∑c d√πng ·∫£nh t√πy ch·ªçn)

Output:
Web app nh·ªè b·∫±ng Streamlit: Ch·ªçn ·∫£nh ‚Üí hi·ªÉn th·ªã caption m√¥ t·∫£

<!-- üéØ D·ª± √°n 2: Visual Question Answering (VQA)
‚úÖ K·∫øt h·ª£p hi·ªÉu ·∫£nh v√† x·ª≠ l√Ω c√¢u h·ªèi vƒÉn b·∫£n
‚öôÔ∏è √Åp d·ª•ng m√¥ h√¨nh ƒëa ph∆∞∆°ng th·ª©c nh∆∞ LLaVA, MiniGPT-4

‚úÖ M·ª•c ti√™u:
Nh·∫≠p v√†o: ·∫£nh + c√¢u h·ªèi

M√¥ h√¨nh tr·∫£ l·ªùi ƒë√∫ng d·ª±a tr√™n n·ªôi dung ·∫£nh

üß∞ C√¥ng c·ª•:
M√¥ h√¨nh: LLaVA ho·∫∑c MiniGPT-4

Streamlit ho·∫∑c Gradio giao di·ªán ng∆∞·ªùi d√πng

üì¶ Output:
Giao di·ªán cho ng∆∞·ªùi d√πng upload ·∫£nh + nh·∫≠p c√¢u h·ªèi ‚Üí hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi

üéØ D·ª± √°n 3: Contrastive Image-Text Search (T√¨m ·∫£nh theo vƒÉn b·∫£n ho·∫∑c ng∆∞·ª£c l·∫°i)
‚úÖ √Åp d·ª•ng contrastive learning + embedding
‚öôÔ∏è G·∫ßn v·ªõi CLIP (OpenAI)

‚úÖ M·ª•c ti√™u:
Nh·∫≠p v√†o ƒëo·∫°n m√¥ t·∫£ ‚Üí t√¨m ra ·∫£nh ph√π h·ª£p nh·∫•t (ho·∫∑c ng∆∞·ª£c l·∫°i)

So kh·ªõp ·∫£nh ‚Äì caption b·∫±ng ƒë·ªô t∆∞∆°ng ƒë·ªìng vector (cosine similarity)

üß∞ C√¥ng c·ª•:
M√¥ h√¨nh: CLIP (openai/clip-vit-base-patch32 t·ª´ Hugging Face)

Dataset: COCO nh·ªè ho·∫∑c ·∫£nh t·ª± thu th·∫≠p

Giao di·ªán t√¨m ki·∫øm b·∫±ng Gradio

üì¶ Output:
T√¨m ki·∫øm ·∫£nh b·∫±ng m√¥ t·∫£ ng·∫Øn (‚Äúm√®o tr√™n b√†n‚Äù) ‚Üí hi·ªÉn th·ªã ·∫£nh ph√π h·ª£p -->
